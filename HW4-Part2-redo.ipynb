{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import cm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.cluster.vq import vq, kmeans, whiten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = {'Brush_teeth':0,\n",
    "            'Climb_stairs': 1,\n",
    "            'Comb_hair':2, \n",
    "            'Descend_stairs':3,\n",
    "            'Drink_glass':4, \n",
    "            'Eat_meat':5,\n",
    "            'Eat_soup':6,\n",
    "            'Getup_bed':7,\n",
    "            'Liedown_bed':8,\n",
    "            'Pour_water':9,\n",
    "            'Sitdown_chair':10,\n",
    "            'Standup_chair':11,\n",
    "            'Use_telephone':12, \n",
    "            'Walk':13\n",
    "            }\n",
    "\n",
    "def loadDataList():\n",
    "    global X_files, activities\n",
    "    for a in activities:\n",
    "        f_list = glob.glob(\"./HMP_Dataset/\" + a + \"/*.txt\")\n",
    "        X_files[activities[a]] = f_list\n",
    "        \n",
    "def splitTrainTest():\n",
    "    global X_train_files, X_test_files\n",
    "    for i, item in enumerate(X_files):\n",
    "        # not using y_train, ytest\n",
    "        X_train_files[i], X_test_files[i], y_train, y_test = train_test_split(X_files[i], [i]*len(X_files[i]), test_size=0.2, random_state=10)\n",
    "\n",
    "def getChunkedVectors(f):\n",
    "    global chunk_size, overlap\n",
    "    df_file = pd.read_table(f, sep=' ', header=None)\n",
    "    df = pd.DataFrame()\n",
    "    index = 0\n",
    "    while index <= len(df_file) - chunk_size:\n",
    "        chunk = df_file.iloc[index: index + 32].values.flatten()\n",
    "        df = df.append([chunk])\n",
    "        index = index + (chunk_size - overlap)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def loadTrainingData():\n",
    "    global X_train_all\n",
    "    for cls, item in enumerate(X_train_files):\n",
    "        df = pd.DataFrame()\n",
    "        for f in X_train_files[cls]:\n",
    "            X_train_all = X_train_all.append(getChunkedVectors(f))            \n",
    "\n",
    "            \n",
    "def computeQuantizedVectors():\n",
    "    global VQ_Train, VQ_Test, codebook\n",
    "    for cls, item in enumerate(X_train_files):\n",
    "        for f in X_train_files[cls]:\n",
    "            df = getChunkedVectors(f)\n",
    "            code, dist = vq(df.astype('float'), codebook)\n",
    "            hist, ed = np.histogram(code, k, density=True)\n",
    "            VQ_Train  = VQ_Train.append([np.append(hist, cls)])\n",
    "\n",
    "    for cls, item in enumerate(X_test_files):\n",
    "        for f in X_test_files[cls]:\n",
    "            df = getChunkedVectors(f)\n",
    "            code, dist = vq(df.astype('float'), codebook)\n",
    "            hist, ed = np.histogram(code, k, density=True)\n",
    "            VQ_Test  = VQ_Test.append([np.append(hist, cls)])\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate test/train in file names only\n",
    "X_files = [None]*len(activities)\n",
    "X_train_files = [None]*len(activities)\n",
    "X_test_files = [None]*len(activities)\n",
    "\n",
    "\n",
    "chunk_size = 32\n",
    "overlap = 16\n",
    "\n",
    "# Dealing with file names only here\n",
    "loadDataList()\n",
    "splitTrainTest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Opening the files and loading all training data for kmeans\n",
    "X_train_all = pd.DataFrame()\n",
    "\n",
    "loadTrainingData()\n",
    "\n",
    "# Running kmeans on training data to find the clusters\n",
    "k = 480\n",
    "codebook, distortion = kmeans(X_train_all.astype(float), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find histogram for training data\n",
    "\n",
    "VQ_Train = pd.DataFrame()\n",
    "VQ_Test = pd.DataFrame()\n",
    "            \n",
    "computeQuantizedVectors()\n",
    "\n",
    "display(VQ_Train.head())\n",
    "display(VQ_Test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26011560693641617"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit to a classifier and check accuracy\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "clf.fit(VQ_Train.drop(k, axis=1), VQ_Train[[k]].values.flatten())\n",
    "\n",
    "predicted = clf.predict(VQ_Test.drop(k, axis=1))\n",
    "accuracy_score(VQ_Test[[k]].values.flatten(), predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
